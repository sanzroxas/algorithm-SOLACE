# -*- coding: utf-8 -*-
"""rf-lstm-hybrid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SsYIYMI7jlhyjxKrb-APmSgZRPnA-21-
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import Input, LSTM, Dense, concatenate, Dropout, BatchNormalization
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)
import joblib
import os
from scipy.stats import randint

# --- Load Data ---
df_train = pd.read_csv("dataset.csv")
df_severity = pd.read_csv("Symptom-severity.csv")

# --- Data Cleaning and Preprocessing ---
def clean_symptom(symptom):
    symptom = str(symptom).strip()
    symptom = " ".join(symptom.split())
    return symptom

for col in df_train.columns:
    if col != "Disease":
        df_train[col] = df_train[col].apply(clean_symptom)
df_severity["Symptom"] = df_severity["Symptom"].apply(clean_symptom)
df_train = df_train.fillna("")
df_severity = df_severity.drop_duplicates(subset="Symptom", keep="first")
df_train = df_train.drop_duplicates()
all_symptoms = sorted(list(df_severity["Symptom"].unique()))
num_symptoms = len(all_symptoms)
print(f"Total unique symptoms: {num_symptoms}")

def prepare_boolean_input(row):
    boolean_vector = [0] * num_symptoms
    for symptom in row:
        symptom = clean_symptom(symptom)
        if symptom in all_symptoms:
            index = all_symptoms.index(symptom)
            boolean_vector[index] = 1
    return boolean_vector

X = np.array(df_train.iloc[:, 1:].apply(prepare_boolean_input, axis=1).tolist())
label_encoder = LabelEncoder()
df_train["Disease_Encoded"] = label_encoder.fit_transform(df_train["Disease"])
y = df_train["Disease_Encoded"].to_numpy()



# --- Model Building (LSTM, RF, Hybrid) ---
def build_hybrid_model(input_shape_lstm, input_shape_rf, num_classes):
    lstm_input = Input(shape=input_shape_lstm)
    lstm_out = LSTM(32, return_sequences=False)(lstm_input)
    lstm_out = Dropout(0.2)(lstm_out)
    lstm_out = BatchNormalization()(lstm_out)

    rf_input = Input(shape=input_shape_rf)
    rf_out = Dense(64, activation="relu")(rf_input)
    rf_out = Dropout(0.2)(rf_out)
    rf_out = BatchNormalization()(rf_out)
    rf_out = Dense(64, activation="relu")(rf_out)
    rf_out = Dropout(0.2)(rf_out)
    rf_out = BatchNormalization()(rf_out)
    rf_out = Dense(32, activation="relu")(rf_out)

    lstm_out = Dense(32, activation='relu')(lstm_out)
    merged = concatenate([lstm_out, rf_out])
    merged = Dense(32, activation="relu")(merged)
    merged = Dropout(0.2)(merged)
    merged = BatchNormalization()(merged)
    output = Dense(num_classes, activation="softmax")(merged)

    model = Model(inputs=[lstm_input, rf_input], outputs=output)
    model.compile(
        optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"]
    )
    return model



# --- Function for Training and Evaluating ---
def train_and_evaluate(X, y, random_state, test_size=0.2):
    """Trains and evaluates both the hybrid and RF models."""

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    print(f"Test set size (random_state={random_state}): {len(X_test)}")
    X_train_lstm = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
    X_test_lstm = X_test.reshape((X_test.shape[0], 1, X_test.shape[1])) # Reshape X_test for LSTM
    X_train_rf = X_train
    X_test_rf = X_test



    # --- Cross-Validation for Hybrid (Optional, but good for comparison) ---
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    for fold_no, (train_index, val_index) in enumerate(kf.split(X_train), 1):

        X_train_fold_lstm, X_val_fold_lstm = X_train_lstm[train_index], X_train_lstm[val_index]
        X_train_fold_rf, X_val_fold_rf = X_train_rf[train_index], X_train_rf[val_index]
        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]

        hybrid_model = build_hybrid_model(
            input_shape_lstm=(X_train_fold_lstm.shape[1], X_train_fold_lstm.shape[2]),
            input_shape_rf=(X_train_fold_rf.shape[1],),
            num_classes=len(label_encoder.classes_),
        )
        history = hybrid_model.fit(
           [X_train_fold_lstm, X_train_fold_rf],
            y_train_fold,
            epochs=50,
            batch_size=32,
            validation_data=([X_val_fold_lstm, X_val_fold_rf], y_val_fold),
            verbose=0,
        )

    # --- Train the Hybrid Model on ALL training data ---
    final_hybrid_model = build_hybrid_model(
        input_shape_lstm=(X_train_lstm.shape[1], X_train_lstm.shape[2]),
        input_shape_rf=(X_train_rf.shape[1],),
        num_classes=len(label_encoder.classes_),
    )
    history = final_hybrid_model.fit([X_train_lstm, X_train_rf], y_train, epochs=50, batch_size=32)



    # --- Hyperparameter Tuning for RF ---
    param_dist = {
        "n_estimators": randint(100, 501),
        "max_depth": [None] + list(randint(10, 41).rvs(size=5)),
        "min_samples_split": randint(2, 11),
        "min_samples_leaf": randint(1, 5),
        "bootstrap": [True, False],
    }
    kf_inner = KFold(n_splits=5, shuffle=True, random_state=42)

    random_search = RandomizedSearchCV(
        estimator=RandomForestClassifier(random_state=42),
        param_distributions=param_dist,
        n_iter=20,
        scoring="accuracy",
        cv=kf_inner,
        n_jobs=-1,
        verbose=0,
        random_state=42,
    )
    random_search.fit(X_train_rf, y_train)
    best_rf_model = random_search.best_estimator_


    # --- Evaluate Hybrid Model on Test Set ---
    hybrid_predictions = final_hybrid_model.predict([X_test_lstm, X_test_rf])
    hybrid_predictions_rf = np.zeros_like(hybrid_predictions)
    y_pred_hybrid = np.argmax((hybrid_predictions+hybrid_predictions_rf)/2, axis=1)

    hybrid_report = classification_report(y_test, y_pred_hybrid, zero_division=0)
    hybrid_accuracy = accuracy_score(y_test, y_pred_hybrid)
    hybrid_precision = precision_score(y_test, y_pred_hybrid, average='weighted', zero_division=0)
    hybrid_recall = recall_score(y_test, y_pred_hybrid, average='weighted', zero_division=0)
    hybrid_f1 = f1_score(y_test, y_pred_hybrid, average='weighted', zero_division=0)


    # --- Evaluate RF Model on Test Set ---
    y_pred_rf = best_rf_model.predict(X_test_rf)

    rf_report = classification_report(y_test, y_pred_rf, zero_division=0)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    rf_precision = precision_score(y_test, y_pred_rf, average="weighted", zero_division=0)
    rf_recall = recall_score(y_test, y_pred_rf, average="weighted", zero_division=0)
    rf_f1 = f1_score(y_test, y_pred_rf, average="weighted", zero_division=0)

    return {
        "hybrid_report": hybrid_report,
        "hybrid_accuracy": hybrid_accuracy,
        "hybrid_precision": hybrid_precision,
        "hybrid_recall": hybrid_recall,
        "hybrid_f1": hybrid_f1,
        "rf_report": rf_report,
        "rf_accuracy": rf_accuracy,
        "rf_precision": rf_precision,
        "rf_recall": rf_recall,
        "rf_f1": rf_f1,
        "best_rf_model": best_rf_model,
        "final_hybrid_model": final_hybrid_model,
    }

# --- Run with Multiple Random States ---
num_runs = 5
hybrid_results = []
rf_results = []
trained_rf_models = []
trained_hybrid_models = []

for i in range(num_runs):
    print(f"\n--- Run {i+1} (Random State: {i}) ---")
    result = train_and_evaluate(X, y, random_state=i)

    hybrid_results.append(
        {
            "accuracy": result["hybrid_accuracy"],
            "precision": result["hybrid_precision"],
            "recall": result["hybrid_recall"],
            "f1": result["hybrid_f1"],
        }
    )
    rf_results.append(
        {
            "accuracy": result["rf_accuracy"],
            "precision": result["rf_precision"],
            "recall": result["rf_recall"],
            "f1": result["rf_f1"],
        }
    )
    trained_rf_models.append(result["best_rf_model"])
    trained_hybrid_models.append(result["final_hybrid_model"])
    print("Hybrid Model Results:")
    print(result["hybrid_report"])
    print("Random Forest Results:")
    print(result["rf_report"])


# --- Analyze Results ---
print("\n--- Summary of Hybrid Model Results ---")
hybrid_accuracies = [res["accuracy"] for res in hybrid_results]
hybrid_precisions = [res["precision"] for res in hybrid_results]
hybrid_recalls = [res["recall"] for res in hybrid_results]
hybrid_f1s = [res["f1"] for res in hybrid_results]

print(f"Accuracies: {hybrid_accuracies}")
print(f"Mean Accuracy: {np.mean(hybrid_accuracies):.4f}, Std Dev: {np.std(hybrid_accuracies):.4f}")
print(f"Mean Precision: {np.mean(hybrid_precisions):.4f}, Std Dev: {np.std(hybrid_precisions):.4f}")
print(f"Mean Recall: {np.mean(hybrid_recalls):.4f}, Std Dev: {np.std(hybrid_recalls):.4f}")
print(f"Mean F1-score: {np.mean(hybrid_f1s):.4f}, Std Dev: {np.std(hybrid_f1s):.4f}")

print("\n--- Summary of Random Forest Results ---")
rf_accuracies = [res["accuracy"] for res in rf_results]
rf_precisions = [res["precision"] for res in rf_results]
rf_recalls = [res["recall"] for res in rf_results]
rf_f1s = [res["f1"] for res in rf_results]

print(f"Accuracies: {rf_accuracies}")
print(f"Mean Accuracy: {np.mean(rf_accuracies):.4f}, Std Dev: {np.std(rf_accuracies):.4f}")
print(f"Mean Precision: {np.mean(rf_precisions):.4f}, Std Dev: {np.std(rf_precisions):.4f}")
print(f"Mean Recall: {np.mean(rf_recalls):.4f}, Std Dev: {np.std(rf_recalls):.4f}")
print(f"Mean F1-score: {np.mean(rf_f1s):.4f}, Std Dev: {np.std(rf_f1s):.4f}")

# --- Save Models and Necessary Objects (from the LAST run) ---
model_dir = "saved_models"
if not os.path.exists(model_dir):
    os.makedirs(model_dir)

# Save the LAST trained models
trained_hybrid_models[-1].save(os.path.join(model_dir, "hybrid_model.h5"))
joblib.dump(trained_rf_models[-1], os.path.join(model_dir, "rf_model.joblib"))
joblib.dump(label_encoder, os.path.join(model_dir, "label_encoder.joblib"))
joblib.dump(all_symptoms, os.path.join(model_dir, "all_symptoms.joblib"))
print(f"Models and preprocessing objects saved to: {model_dir}")


# --- Prediction Function (with loading and averaging) ---
def predict_disease(input_symptoms, model_dir="saved_models"):
    try:
        hybrid_model = load_model(os.path.join(model_dir, "hybrid_model.h5"))
        rf_model = joblib.load(os.path.join(model_dir, "rf_model.joblib"))
        label_encoder = joblib.load(os.path.join(model_dir, "label_encoder.joblib"))
        all_symptoms = joblib.load(os.path.join(model_dir, "all_symptoms.joblib"))
        num_symptoms = len(all_symptoms)

        if isinstance(input_symptoms, list):
            if all(isinstance(item, bool) for item in input_symptoms):
                if len(input_symptoms) != num_symptoms:
                    raise ValueError(
                        f"Boolean input list must have length {num_symptoms} (all symptoms)."
                    )
                boolean_input = input_symptoms
            elif all(isinstance(item, str) for item in input_symptoms):
                boolean_input = [0] * num_symptoms
                for symptom in input_symptoms:
                    symptom = clean_symptom(symptom)
                    if symptom in all_symptoms:
                        index = all_symptoms.index(symptom)
                        boolean_input[index] = 1
            else:
                raise TypeError("Input symptoms must be a list of booleans or a list of strings.")
        else:
            raise TypeError("Input symptoms must be a list.")

        input_rf = np.array(boolean_input).reshape(1, -1)
        input_lstm = input_rf.reshape(1, 1, -1)

        lstm_pred = hybrid_model.predict([input_lstm, input_rf])
        rf_pred = rf_model.predict_proba(input_rf)
        combined_pred = (lstm_pred + rf_pred) / 2

        disease_probabilities = dict(zip(label_encoder.classes_, combined_pred[0]))
        sorted_probabilities = dict(
            sorted(disease_probabilities.items(), key=lambda item: item[1], reverse=True)
        )
        return sorted_probabilities

    except Exception as e:
        print(f"Error during prediction: {e}")
        return None

# --- Example Usage ---
selected_symptoms = ["itching", "skin_rash", "nodal_skin_eruptions"]
predicted_diseases = predict_disease(selected_symptoms)
if predicted_diseases:
    print("\nPredicted Disease Probabilities (Selected Symptoms):")
    for disease, probability in predicted_diseases.items():
        print(f"{disease}: {probability:.4f}")

boolean_symptoms = [0] * num_symptoms
boolean_symptoms[all_symptoms.index("itching")] = 1
boolean_symptoms[all_symptoms.index("skin_rash")] = 1
boolean_symptoms[all_symptoms.index("nodal_skin_eruptions")] = 1
predicted_diseases = predict_disease(boolean_symptoms)
if predicted_diseases:
    print("\nPredicted Disease Probabilities (Boolean Input):")
    for disease, probability in predicted_diseases.items():
        print(f"{disease}: {probability:.4f}")

invalid_boolean_input = [0] * (num_symptoms - 1)
predicted_diseases = predict_disease(invalid_boolean_input)
invalid_mixed_input = ["itching", 0, 1]
predicted_diseases = predict_disease(invalid_mixed_input)
unknown_symptom_input = ["itching", "skin_rash", "unknown_symptom"]
predicted_diseases = predict_disease(unknown_symptom_input)
if predicted_diseases:
    print("\nPredicted Disease Probabilities (Unknown Symptom):")
    for disease, probability in predicted_diseases.items():
        print(f"{disease}: {probability:.4f}")